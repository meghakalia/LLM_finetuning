{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/meghakalia/LLM_finetuning/blob/main/SFT_finetuning_LinkedinPosts.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YTW5u41Ilg0s",
        "outputId": "486a24cc-2a0b-49ab-9303-a7af780efbb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sun Mar  1 01:25:01 2026       \n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 580.82.07              Driver Version: 580.82.07      CUDA Version: 13.0     |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                        |               MIG M. |\n",
            "|=========================================+========================+======================|\n",
            "|   0  Tesla T4                       Off |   00000000:00:04.0 Off |                    0 |\n",
            "| N/A   60C    P8             10W /   70W |       0MiB /  15360MiB |      0%      Default |\n",
            "|                                         |                        |                  N/A |\n",
            "+-----------------------------------------+------------------------+----------------------+\n",
            "\n",
            "+-----------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                              |\n",
            "|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |\n",
            "|        ID   ID                                                               Usage      |\n",
            "|=========================================================================================|\n",
            "|  No running processes found                                                             |\n",
            "+-----------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eoxXXTAAloeq"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dyqMVtgMCgzy",
        "outputId": "c85d0224-31f6-4b9d-bd86-3fd3896db185"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x_9SeDN6ls34"
      },
      "outputs": [],
      "source": [
        "!pip -q install --upgrade pip\n",
        "!pip -q install \"unsloth\" trl transformers datasets accelerate bitsandbytes peft"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-CNatvm90Rx-"
      },
      "source": [
        "## Helper functions & Test Prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vmL7Kenm0UeS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import math\n",
        "from torch.nn import CrossEntropyLoss\n",
        "\n",
        "def compute_perplexity(model, tokenizer, dataset, max_samples=20):\n",
        "    model.eval()\n",
        "    losses = []\n",
        "\n",
        "    for i, sample in enumerate(dataset):\n",
        "        if i >= max_samples:  # keep evaluation fast\n",
        "            break\n",
        "\n",
        "        inputs = tokenizer(sample[\"text\"], return_tensors=\"pt\", truncation=True).to(\"cuda\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
        "            loss = outputs.loss\n",
        "            losses.append(loss.item())\n",
        "\n",
        "    avg_loss = sum(losses) / len(losses)\n",
        "    ppl = math.exp(avg_loss)\n",
        "    return avg_loss, ppl"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gho_tM4M0ZDg"
      },
      "outputs": [],
      "source": [
        "test_prompt = \"\"\"### Instruction:\n",
        "Write a LinkedIn post about burnout in startups. 5â€“8 lines. No emojis. End with a question.\n",
        "\n",
        "### Response:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DpH3CSNE0PIL"
      },
      "source": [
        "## Load base model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "8d39c04ffaef46b7b37020b2a31560e7",
            "2bf1e707202048549e8db07835a3c723",
            "f7133f78d18a494eac2d9f1e5e95100c",
            "8b0ca3aa66fa4618b14fbb76b175f50f",
            "72e23a6efd5c412f852d5b9cee062fea",
            "4502436f2f7a425599439e1ff4c36219",
            "8cb526c2716a4b28a53f611c6feff11e",
            "d871016b2be44e329ff92cf2ce3b644a",
            "69a189dd99e744acbd62b6a5ebed66f3",
            "a70dbdcd1e3c4fd2a6434ad22e5a868b",
            "a38afeacaf0c44588fd889095f7a0242"
          ]
        },
        "collapsed": true,
        "id": "X3iJxDZIl5Sv",
        "outputId": "053fd88c-de1c-4e9d-a989-162b7a8d9574"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n",
            "==((====))==  Unsloth 2026.2.1: Fast Qwen2 patching. Transformers: 4.57.6.\n",
            "   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.563 GB. Platform: Linux.\n",
            "O^O/ \\_/ \\    Torch: 2.10.0+cu128. CUDA: 7.5. CUDA Toolkit: 12.8. Triton: 3.6.0\n",
            "\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.35. FA2 = False]\n",
            " \"-____-\"     Free license: http://github.com/unslothai/unsloth\n",
            "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8d39c04ffaef46b7b37020b2a31560e7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\n",
            "Unsloth will patch all other layers, except LoRA matrices, causing a performance hit.\n",
            "Unsloth 2026.2.1 patched 28 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "PeftModelForCausalLM(\n",
              "  (base_model): LoraModel(\n",
              "    (model): Qwen2ForCausalLM(\n",
              "      (model): Qwen2Model(\n",
              "        (embed_tokens): Embedding(152064, 3584, padding_idx=151654)\n",
              "        (layers): ModuleList(\n",
              "          (0-1): 2 x Qwen2DecoderLayer(\n",
              "            (self_attn): Qwen2Attention(\n",
              "              (q_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=3584, out_features=3584, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=3584, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=3584, out_features=512, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=3584, out_features=512, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=3584, out_features=3584, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=3584, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): Qwen2MLP(\n",
              "              (gate_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=18944, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=18944, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=18944, out_features=3584, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=18944, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=3584, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "            (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "          )\n",
              "          (2-4): 3 x Qwen2DecoderLayer(\n",
              "            (self_attn): Qwen2Attention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=3584, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=3584, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): Qwen2MLP(\n",
              "              (gate_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=3584, out_features=18944, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=18944, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=3584, out_features=18944, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=18944, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=18944, out_features=3584, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=18944, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=3584, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "            (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "          )\n",
              "          (5-24): 20 x Qwen2DecoderLayer(\n",
              "            (self_attn): Qwen2Attention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=3584, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=3584, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): Qwen2MLP(\n",
              "              (gate_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=18944, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=18944, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=18944, out_features=3584, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=18944, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=3584, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "            (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "          )\n",
              "          (25-26): 2 x Qwen2DecoderLayer(\n",
              "            (self_attn): Qwen2Attention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=3584, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=3584, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): Qwen2MLP(\n",
              "              (gate_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=3584, out_features=18944, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=18944, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=3584, out_features=18944, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=18944, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear(\n",
              "                (base_layer): Linear(in_features=18944, out_features=3584, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=18944, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=3584, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "            (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "          )\n",
              "          (27): Qwen2DecoderLayer(\n",
              "            (self_attn): Qwen2Attention(\n",
              "              (q_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=3584, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (k_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (v_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=512, bias=True)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=512, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (o_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=3584, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=3584, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (rotary_emb): LlamaRotaryEmbedding()\n",
              "            )\n",
              "            (mlp): Qwen2MLP(\n",
              "              (gate_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=18944, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (up_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=3584, out_features=18944, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=3584, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=18944, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (down_proj): lora.Linear4bit(\n",
              "                (base_layer): Linear4bit(in_features=18944, out_features=3584, bias=False)\n",
              "                (lora_dropout): ModuleDict(\n",
              "                  (default): Dropout(p=0.05, inplace=False)\n",
              "                )\n",
              "                (lora_A): ModuleDict(\n",
              "                  (default): Linear(in_features=18944, out_features=16, bias=False)\n",
              "                )\n",
              "                (lora_B): ModuleDict(\n",
              "                  (default): Linear(in_features=16, out_features=3584, bias=False)\n",
              "                )\n",
              "                (lora_embedding_A): ParameterDict()\n",
              "                (lora_embedding_B): ParameterDict()\n",
              "                (lora_magnitude_vector): ModuleDict()\n",
              "              )\n",
              "              (act_fn): SiLUActivation()\n",
              "            )\n",
              "            (input_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "            (post_attention_layernorm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "          )\n",
              "        )\n",
              "        (norm): Qwen2RMSNorm((3584,), eps=1e-06)\n",
              "        (rotary_emb): LlamaRotaryEmbedding()\n",
              "      )\n",
              "      (lm_head): Linear(in_features=3584, out_features=152064, bias=False)\n",
              "    )\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "from unsloth import FastLanguageModel\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from unsloth import FastLanguageModel\n",
        "from trl import SFTTrainer\n",
        "from transformers import TrainingArguments\n",
        "\n",
        "MODEL_NAME = \"unsloth/Qwen2.5-7B-Instruct\"\n",
        "MAX_SEQ_LEN = 2048\n",
        "\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name = MODEL_NAME,\n",
        "    max_seq_length = MAX_SEQ_LEN,\n",
        "    load_in_4bit = True,\n",
        "    dtype = None,\n",
        "    device_map=None\n",
        "\n",
        ")\n",
        "\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r = 16,# lora rank, reduce to prevent overfitting\n",
        "    target_modules = [\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\"],\n",
        "    lora_alpha = 16,\n",
        "    lora_dropout = 0.05,\n",
        "    bias = \"none\",\n",
        "    use_gradient_checkpointing = \"unsloth\",\n",
        ")\n",
        "\n",
        "model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gaBisU_yWxR"
      },
      "source": [
        "## Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103,
          "referenced_widgets": [
            "b1afefbc1cf84545a852b732591c06b1",
            "b2f334b25e584774b5dca0e656e47bf6",
            "f74851a85f80489283ff522b3d2d4460",
            "68929adce66f4289a3d8aa67adf43855",
            "48d644895355416f9b174fc1f0d64460",
            "dba82da92f9d4a5b820b2744d301e565",
            "6aa852d66d8c41ef880d675ec1d11602",
            "b63cd45952a9410cb332a5a6c5e967cb",
            "5dd1982f6f904acfbee9236883cf02de",
            "bd148f563a0a4d2882f8c819e908bd67",
            "b118232ec4ef445eaac7353200e91f7a"
          ]
        },
        "id": "_zYdReQQl5aI",
        "outputId": "34577020-75f0-4bef-8d1c-dbe54789f9c0"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b1afefbc1cf84545a852b732591c06b1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/49 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train: 39\n",
            "Val: 5\n",
            "Test: 5\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "ds = load_dataset(\"json\", data_files=\"sample_data/train.jsonl\", split=\"train\")\n",
        "\n",
        "def to_text(ex):\n",
        "    ex[\"text\"] = f\"\"\"### Instruction:{ex['instruction']}### Response:{ex['output']}{tokenizer.eos_token}\"\"\"\n",
        "    return ex\n",
        "\n",
        "\n",
        "ds = ds.map(to_text)\n",
        "\n",
        "# 80/10/10 split\n",
        "split1 = ds.train_test_split(test_size=0.2, seed=42)\n",
        "train_ds = split1[\"train\"]\n",
        "temp_ds = split1[\"test\"]\n",
        "\n",
        "split2 = temp_ds.train_test_split(test_size=0.5, seed=42)\n",
        "val_ds = split2[\"train\"]\n",
        "test_ds = split2[\"test\"]\n",
        "\n",
        "print(\"Train:\", len(train_ds))\n",
        "print(\"Val:\", len(val_ds))\n",
        "print(\"Test:\", len(test_ds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EWQVhCFOyZiW"
      },
      "source": [
        "## Perplexity before Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFs9uktkyPnW",
        "outputId": "074d95e8-5b1a-4011-f7f1-9690a0d7930f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating BASE model...\n",
            "Base Test Loss: 5.0795\n",
            "Base Test Perplexity: 160.6977\n"
          ]
        }
      ],
      "source": [
        "print(\"Evaluating BASE model...\")\n",
        "\n",
        "base_loss, base_ppl = compute_perplexity(model, tokenizer, test_ds)\n",
        "\n",
        "print(f\"Base Test Loss: {base_loss:.4f}\")\n",
        "print(f\"Base Test Perplexity: {base_ppl:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "suiXDpoW1K-k",
        "outputId": "a91669b7-ca93-4015-bad8-d0f7d17e6c38"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "### Instruction:\n",
            "Write a LinkedIn post about burnout in startups. 5â€“8 lines. No emojis. End with a question.\n",
            "\n",
            "### Response:\n",
            "In the fast-paced world of startups, burnout is a silent epidemic, often masked by the relentless pursuit of innovation and growth. Founders and early employees frequently push beyond sustainable limits to meet tight deadlines and ambitious goals. This relentless pressure can lead to chronic stress, exhaustion, and a decline in personal well-being. It's crucial for startups to prioritize mental health and foster an environment where work-life balance is not just a buzzword but a reality. How can startups create sustainable practices to prevent burnout among their teams? \n",
            "\n",
            "[End]\n"
          ]
        }
      ],
      "source": [
        "device = next(model.parameters()).device\n",
        "\n",
        "prompt = test_prompt\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    out = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=200,\n",
        "        temperature=0.8,\n",
        "    )\n",
        "\n",
        "print(tokenizer.decode(out[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5N2XtySFydUC"
      },
      "source": [
        "## Training with Grad accumulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 528,
          "referenced_widgets": [
            "df0eeeb53dfa4d1f9081bef98861ca12",
            "e201d01f931848219ccee585a6f2a07c",
            "846d66d111e54b019ae8b6c3e9fc4559",
            "fa5336b20d8142829b43f50c973a6e4b",
            "a31d249aba1749cd962853a3c1e0db41",
            "c817b903c63a48ecb8937ff687628cbe",
            "c5804ebb7ecb4104a04813cc151f8186",
            "f800d985fb4041208aa9f0e94f66c208",
            "e1fffc4223ab439d92c1f7f4f3b9b863",
            "1d627863000145a2b4f81914e9f14c9a",
            "9823e382f755473fae07646655c3c15a",
            "6b15139d187c4d81aa77b91e793c8a21",
            "5835ba9b8f4e4ed297a6706abea93671",
            "9317e9a3c452422093f7a999842c9f25",
            "ad67e90d28c3490fa2092fb13db06ac8",
            "2cde476893134935bbe5b70191468764",
            "435bb57cc93949d6994664d75838f22a",
            "a1ed77dd11c14b728e0cb2ab074d4973",
            "6bdac414a703452cb9ccfb12a61c6066",
            "7bff6b0a79ce452883b973c4b3ce066e",
            "78351d07493b4e8ba83d80885ca4c6f1",
            "dea0e324a08d483f8d1101c17ad6440b"
          ]
        },
        "id": "vkHl37nctAzT",
        "outputId": "ddcb245e-d9fa-469c-fe34-fd048632e218"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "df0eeeb53dfa4d1f9081bef98861ca12",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=6):   0%|          | 0/39 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "num_proc must be <= 5. Reducing num_proc to 5 for dataset of size 5.\n",
            "WARNING:datasets.arrow_dataset:num_proc must be <= 5. Reducing num_proc to 5 for dataset of size 5.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6b15139d187c4d81aa77b91e793c8a21",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Unsloth: Tokenizing [\"text\"] (num_proc=5):   0%|          | 0/5 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n",
            "   \\\\   /|    Num examples = 39 | Num Epochs = 1 | Total steps = 5\n",
            "O^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n",
            "\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n",
            " \"-____-\"     Trainable parameters = 40,370,176 of 7,655,986,688 (0.53% trained)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [5/5 00:19, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>4.805500</td>\n",
              "      <td>5.014990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>4.885000</td>\n",
              "      <td>4.869985</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>4.625500</td>\n",
              "      <td>4.757622</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>4.580400</td>\n",
              "      <td>4.687138</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>4.489800</td>\n",
              "      <td>4.652585</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Unsloth: Not an error, but Qwen2ForCausalLM does not accept `num_items_in_batch`.\n",
            "Using gradient accumulation will be very slightly less accurate.\n",
            "Read more on gradient accumulation issues here: https://unsloth.ai/blog/gradient\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "TrainOutput(global_step=5, training_loss=4.677258586883545, metrics={'train_runtime': 24.7143, 'train_samples_per_second': 1.578, 'train_steps_per_second': 0.202, 'total_flos': 112211411343360.0, 'train_loss': 4.677258586883545, 'epoch': 1.0})"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "args = TrainingArguments(\n",
        "    output_dir=\"out_lora\",\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=4,\n",
        "    learning_rate=5e-5,\n",
        "    num_train_epochs=1,  # increase to see loss curve\n",
        "    weight_decay=0.01, # the current model is overfitting\n",
        "    logging_steps=1,\n",
        "    eval_strategy=\"steps\",   # important\n",
        "    eval_steps=1,                 # evaluate every 10 steps\n",
        "    save_strategy=\"no\",\n",
        "    fp16=not torch.cuda.is_bf16_supported(),\n",
        "    bf16=torch.cuda.is_bf16_supported(),\n",
        "    optim=\"adamw_8bit\",\n",
        "    report_to=\"none\",\n",
        ")\n",
        "\n",
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    tokenizer=tokenizer,\n",
        "    train_dataset=train_ds,\n",
        "    eval_dataset=val_ds,\n",
        "    dataset_text_field=\"text\",\n",
        "    max_seq_length=MAX_SEQ_LEN,\n",
        "    args=args,\n",
        ")\n",
        "\n",
        "trainer.train()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D73lgBeTym0G"
      },
      "source": [
        "## Train-Val Loss Log Loss Curve"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 508
        },
        "id": "MJtj27xuuQxh",
        "outputId": "b4268989-2f69-4f90-d27b-53f745feab78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Final Train Loss: 4.4898\n",
            "Final Validation Loss: 4.652585029602051\n"
          ]
        },
        {
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "metrics = trainer.state.log_history\n",
        "\n",
        "train_losses = [x[\"loss\"] for x in metrics if \"loss\" in x]\n",
        "eval_losses = [x[\"eval_loss\"] for x in metrics if \"eval_loss\" in x]\n",
        "\n",
        "print(\"Final Train Loss:\", train_losses[-1])\n",
        "print(\"Final Validation Loss:\", eval_losses[-1])\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(train_losses, label=\"Train Loss\")\n",
        "plt.plot(eval_losses, label=\"Validation Loss\")\n",
        "plt.legend()\n",
        "plt.xlabel(\"Logging Step\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.title(\"Training vs Validation Loss\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HuctBdPMyJOF",
        "outputId": "bf5c668d-8150-4635-ff26-f48e1c2f7516"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating FINE-TUNED model...\n",
            "Fine-Tuned Test Loss: 4.6704\n",
            "Fine-Tuned Test Perplexity: 106.7357\n"
          ]
        }
      ],
      "source": [
        "print(\"Evaluating FINE-TUNED model...\")\n",
        "\n",
        "ft_loss, ft_ppl = compute_perplexity(model, tokenizer, test_ds)\n",
        "\n",
        "print(f\"Fine-Tuned Test Loss: {ft_loss:.4f}\")\n",
        "print(f\"Fine-Tuned Test Perplexity: {ft_ppl:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "za88bR7Czq6o",
        "outputId": "e3670935-f93b-4621-99a4-d1795c99b457"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== FINAL COMPARISON ===\n",
            "Base Perplexity:        160.6977\n",
            "Fine-Tuned Perplexity:  106.7357\n",
            "Improvement:            53.9621\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n=== FINAL COMPARISON ===\")\n",
        "print(f\"Base Perplexity:        {base_ppl:.4f}\")\n",
        "print(f\"Fine-Tuned Perplexity:  {ft_ppl:.4f}\")\n",
        "print(f\"Improvement:            {base_ppl - ft_ppl:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38jI3mEkzwSG"
      },
      "source": [
        "## Quantitative Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D7ox3JLmzug9"
      },
      "outputs": [],
      "source": [
        "test_prompt = \"\"\"### Instruction:\n",
        "Write a LinkedIn post about burnout in startups. 5â€“8 lines. No emojis. End with a question.\n",
        "\n",
        "### Response:\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8Ftzj13z3qT"
      },
      "source": [
        "### Generate with Base Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X51GIk2Iz-JS"
      },
      "source": [
        "### Generate with finetuned model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hWy-uIZc0A7U",
        "outputId": "ddbd2015-bbe8-464c-aa47-faf149665259"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== FINE-TUNED MODEL OUTPUT ===\n",
            "\n",
            "### Instruction:\n",
            "Write a LinkedIn post about burnout in startups. 5â€“8 lines. No emojis. End with a question.\n",
            "\n",
            "### Response:\n",
            "In the fast-paced world of startups, burnout is a silent but pervasive threat to both individual well-being and company success. Founders and employees often push beyond sustainable limits, driven by the relentless pursuit of growth and innovation. Yet, this relentless drive can lead to chronic stress, diminished productivity, and even physical health issues. It's crucial for startups to prioritize mental health, fostering a culture that values sustainable progress over rapid expansion. How can startups create healthier work environments to prevent burnout? \n",
            "\n",
            "Question: How can startups balance growth with employee well-being to combat burnout?\n"
          ]
        }
      ],
      "source": [
        "FastLanguageModel.for_inference(model)\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    out = model.generate(**inputs, max_new_tokens=200, temperature=0.8)\n",
        "\n",
        "print(\"\\n=== FINE-TUNED MODEL OUTPUT ===\\n\")\n",
        "print(tokenizer.decode(out[0], skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 454
        },
        "id": "0s4Z43GtBM6l",
        "outputId": "af84c6a4-c46d-486c-d153-41c5c2ded5f1"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/SFT_finetuning_LinkedinPosts.ipynb'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nbformat/__init__.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(fp, as_version, capture_validation_error, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'read'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-32988/3645607969.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mnotebook_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/content/SFT_finetuning_LinkedinPosts.ipynb\"\u001b[0m  \u001b[0;31m# <-- CHANGE THIS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mnb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnbformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnotebook_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_version\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Remove ONLY widget metadata (keep outputs)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nbformat/__init__.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(fp, as_version, capture_validation_error, **kwargs)\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0mbuf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"utf8\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# noqa: PTH123\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mreads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mas_version\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcapture_validation_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/SFT_finetuning_LinkedinPosts.ipynb'"
          ]
        }
      ],
      "source": [
        "import nbformat\n",
        "from google.colab import drive\n",
        "import os\n",
        "\n",
        "# Find current notebook path\n",
        "notebook_name = os.path.basename(\"/content\")  # fallback\n",
        "\n",
        "# If using default Colab name, replace this manually:\n",
        "notebook_path = \"/content/SFT_finetuning_LinkedinPosts.ipynb\"  # <-- CHANGE THIS\n",
        "\n",
        "nb = nbformat.read(notebook_path, as_version=4)\n",
        "\n",
        "# Remove ONLY widget metadata (keep outputs)\n",
        "if \"widgets\" in nb.metadata:\n",
        "    del nb.metadata[\"widgets\"]\n",
        "\n",
        "nbformat.write(nb, notebook_path)\n",
        "\n",
        "print(\"âœ… Widget metadata removed. All outputs preserved.\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyPzamb948gKk5f5KLO9HmLp",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}